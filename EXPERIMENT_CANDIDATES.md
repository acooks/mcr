# Experiment Candidates - High-Risk Architecture Components

This document identifies complex, unproven, or high-risk technical challenges in the architecture that would benefit from isolated proof-of-concept experiments before integration into the main application.

**Date:** 2025-11-07

---

## Prioritization Criteria

Experiments are prioritized by:
1. **Technical Risk:** Unproven or complex integration
2. **Performance Criticality:** On the hot path for packet processing
3. **Integration Complexity:** Multiple subsystems must work together
4. **Likelihood of Failure:** High chance of unexpected behavior

**Priority Levels:**
- üî¥ **Critical** - Blocks core functionality, high risk of failure
- üü° **High** - Important feature, moderate risk
- üü¢ **Medium** - Nice-to-have validation, lower risk

---

## üî¥ Critical Priority Experiments

### 1. ‚úÖ Helper Socket Pattern for IGMP + NIC Filtering (D6, D4) - COMPLETED

**Status:** ‚úÖ **VALIDATED** (2025-11-07)

**Problem:**
The design relies on a non-obvious pattern:
- Create `AF_INET` socket solely to trigger IGMP Join
- Set `SO_RCVBUF` to minimum (socket never read)
- Actual packets received via separate `AF_PACKET` socket on same interface
- NIC MAC filtering programmed by kernel based on unused `AF_INET` socket

**Original Unproven Assumptions:**
- Does the kernel maintain IGMP membership for a socket that's never read?
- Does the NIC's MAC address filter work correctly with this split-socket pattern?
- What happens to packets that land in the `AF_INET` socket buffer? Do they cause errors?
- How does this interact with interface up/down events?

**Why Critical:**
If this doesn't work, **the entire ingress filtering strategy collapses**. Packets won't reach the `AF_PACKET` socket.

**Experiment Results:**

‚úÖ **SUCCESS - Pattern is viable!**

The experiment (`experiments/poc_helper_socket_igmp/`) definitively proved:

1. ‚úÖ **IGMP Join Works** - AF_INET helper socket successfully triggers IGMP join
2. ‚úÖ **AF_PACKET Receives** - Packets arrive at the AF_PACKET socket on the same interface
3. ‚úÖ **Helper Socket Empty** - Helper socket has no readable data (as designed)
4. ‚úÖ **Separation Works** - The two sockets operate independently

**Key Findings:**
- Kernel maintains multicast group membership even for unread sockets
- NIC MAC filtering is programmed correctly from the helper socket's IGMP join
- Helper socket buffer remains empty (SO_RCVBUF=2304 bytes, no data)
- AF_PACKET socket received all interface traffic (IPv6, IGMP, and would receive IPv4 UDP)
- No interference between helper socket and AF_PACKET socket

**Implementation Notes:**
- Used `socket.into_raw_fd()` (not `as_raw_fd()`) to transfer ownership
- Helper socket must remain open for IGMP membership to persist
- AF_PACKET with `ETH_P_ALL` captures everything; use BPF filter or `ETH_P_IP` in production
- Userspace demultiplexing needed to filter by UDP + dest IP/port

**Related Designs:** D6 (Helper Socket Pattern), D4 (Hardware Filtering), D3 (Userspace Demux)

**Architectural Impact:** **CRITICAL** - Core ingress path design is VALIDATED ‚úÖ

**Next Steps:**
- Proceed with data plane implementation (Phase 4)
- No architectural redesign needed for ingress filtering
- Consider interface up/down event testing (future experiment)

---

### 2. ‚úÖ File Descriptor Passing with Privilege Drop (D24) - COMPLETED

**Status:** ‚úÖ **VALIDATED** (2025-11-07)

**Problem:**
The multi-process privilege separation model requires:
- Supervisor (privileged) creates `AF_PACKET` sockets
- Passes sockets to unprivileged worker processes via Unix domain socket + `SCM_RIGHTS`
- Workers use sockets without ever having `CAP_NET_RAW`

**Unproven Assumptions:**
- Can an `AF_PACKET` socket created with `CAP_NET_RAW` continue to work after being passed to an unprivileged process?
- Does the socket retain its binding and capture mode after passing?
- What about socket options (e.g., ring buffer size, PACKET_VERSION)?
- Can the worker call `recvfrom()` or use `io_uring` with the passed FD?

**Why Critical:**
If FD passing doesn't preserve socket capabilities, **the entire privilege separation architecture fails**.

**Experiment Design:**
1. Privileged parent process:
   - Create `AF_PACKET` socket with `ETH_P_ALL`
   - Bind to specific interface
   - Set ring buffer options
2. Fork/spawn unprivileged child process (drop `CAP_NET_RAW`)
3. Pass socket FD via `SCM_RIGHTS` over Unix domain socket
4. Child process:
   - Receive FD
   - Attempt to receive packets using `recvfrom()`
   - Attempt to use FD with `io_uring` operations
5. Verify packets are received correctly in child

**Related Designs:** D24, D1, D7

**Architectural Impact:** **CRITICAL** - Validates security architecture

**üìä Results:** ‚úÖ **VALIDATED** (2025-11-07) - See `experiments/poc_fd_passing_privdrop/`

**Validated Assumptions:**
1. ‚úÖ **Socket Capabilities Survive** - AF_PACKET socket created with CAP_NET_RAW continues functioning after being passed to unprivileged process (UID/GID 65534)
2. ‚úÖ **FD Passing Works** - SCM_RIGHTS successfully transfers socket FD via Unix domain socketpair
3. ‚úÖ **Privilege Drop Complete** - Child process successfully drops all privileges and cannot create new AF_PACKET sockets
4. ‚úÖ **Packet Reception Works** - Unprivileged child received 5/5 test packets using passed socket

**Key Findings:**
- Socket binding and capture mode survive FD passing
- Unprivileged process can call `recvfrom()` on passed AF_PACKET socket
- Privilege drop is complete and irreversible (CAP_NET_RAW verified gone)
- FD numbering changes across processes (as expected)

**Implementation Notes:**
- nix 0.30 API: `sendmsg()`/`recvmsg()` accept raw `i32` FDs (not `BorrowedFd`)
- `msg.cmsgs()` returns `Result<CmsgIterator>` requiring `.context()?` handling
- Fork safety: Parent/child must close opposite socket ends
- Network namespace testing provides isolated environment

**Related Designs:** D24 (Privilege Separation), D1 (AF_PACKET), D18 (Supervisor Pattern)

**Architectural Impact:** **CRITICAL** - Security architecture is VALIDATED ‚úÖ

**Next Steps:**
- Proceed with Phase 2-3 implementation (supervisor and worker processes)
- Workers can safely run as unprivileged users (e.g., `mcrelay:mcrelay`)
- Still need Experiment #3 to validate io_uring with passed FDs

---

### 3. ‚úÖ Core-Local Buffer Pool Performance (D15, D16) - COMPLETED

**Status:** ‚úÖ **VALIDATED** (2025-11-07)

**Problem:**
The design requires custom memory management:
- Multiple size-based pools (Small, Standard, Jumbo) per core
- Lock-free allocation/deallocation (single-threaded per core)
- Exhaustion handling (drop packets, no dynamic fallback)
- Per-pool metrics tracking

**Original Unproven Assumptions:**
- What's the actual performance of pool operations vs. `Vec::with_capacity()`?
- How much memory is needed per core for realistic workloads?
- What's the CPU overhead of tracking metrics per allocation/deallocation?
- How does pool exhaustion behavior affect throughput under load?

**Why Critical:**
This is **on the hot path for every packet**. Poor design = performance catastrophe.

**Experiment Results:**

‚úÖ **SUCCESS - Buffer pools validated!**

The experiment (`experiments/poc_buffer_pool_performance/`) definitively proved:

1. ‚úÖ **Performance Advantage** - Pool 1.79x faster than Vec (26.7 ns vs 47.7 ns)
2. ‚úÖ **Massive Headroom** - 37.6M ops/sec (120x over 312.5k pps/core target)
3. ‚úÖ **Graceful Exhaustion** - 98-99% success rate under 2-10x bursts, no crashes
4. ‚úÖ **Fast Recovery** - 0.8-3.5 ¬µs recovery time (25,000x faster than 100ms target)
5. ‚úÖ **Free Metrics** - <3% overhead with full per-operation tracking

**Key Findings:**
- Pool allocation latency: 26.7 ns (target: <50 ns) - 46% better than target
- Throughput: 37.6M ops/sec (target: >5M) - 7.5x better than target
- Recommended pool size: 1000/500/200 buffers per core (5.3 MB/core, 85 MB for 16 cores)
- Memory management is NOT a bottleneck - massive CPU budget available
- Metrics tracking has negligible overhead - safe to enable in production

**Related Designs:** D15, D16, D5

**Architectural Impact:** **CRITICAL** - Core memory management strategy VALIDATED ‚úÖ

**Next Steps:**
- Proceed with Phase 4 data plane implementation using validated pool design
- Use 1000/500/200 buffer configuration per core
- Enable metrics by default (zero overhead)

---

## üü° High Priority Experiments

### 4. Packet Header Parsing Performance (D3, D11, D30)

**Problem:**
Every packet requires parsing:
- Ethernet header (14 bytes) - extract EtherType, dest MAC
- IPv4 header (20+ bytes) - extract src/dst IP, protocol, flags, fragment offset
- UDP header (8 bytes) - extract src/dst port
- Fragment detection (D30) - check MF flag and offset

**Unproven Assumptions:**
- What's the parsing overhead per packet?
- Is manual byte manipulation faster than using a crate like `pnet_packet`?
- How much does fragment detection add to the critical path?
- Cache line alignment impact?

**Why High Priority:**
**On the hot path for every packet**. Even 100ns overhead = 100¬µs at 1M pps.

**Experiment Design:**
1. Capture diverse real packet samples (normal UDP, fragments, jumbo frames)
2. Implement three parsing approaches:
   - Raw `unsafe` pointer manipulation
   - Safe Rust with slice indexing
   - Using `pnet_packet` crate
3. Benchmark each approach:
   - Parse 10M packets in memory
   - Measure latency (avg, p50, p99)
   - Profile with `perf` for cache misses

**Related Designs:** D3, D11, D30, D32

**Architectural Impact:** Data plane performance

---

### 5. ‚úÖ Batched io_uring sendto() for Egress (D8, D5, D26) - COMPLETED

**Status:** ‚úÖ **VALIDATED** (2025-11-07)

**Problem:**
Egress uses `io_uring` for batched `sendto()` operations:
- Submit multiple sends to submission queue
- Reap completions in batch
- Handle transient errors (D26)
- Bind socket to specific source IP

**Original Unproven Assumptions:**
- What batch size gives optimal throughput?
- How do egress errors (EHOSTUNREACH, EAGAIN) manifest in `io_uring`?
- Can we bind `AF_INET` `UdpSocket` to source IP and still use `sendto()` to specify dest?
- What's the overhead vs. raw `sendmsg()`?

**Why High Priority:**
Egress batching is critical for **millions of packets per second** throughput.

**Experiment Results:**

‚úÖ **SUCCESS - io_uring batching validated!**

The experiment (`experiments/poc_io_uring_egress/`) definitively proved:

1. ‚úÖ **Batching Works** - 32x syscall reduction (1.85M sendto() ‚Üí 57k io_uring_enter())
2. ‚úÖ **Optimal Batch Size** - 32-64 packets (throughput plateaus at 1.85M pps)
3. ‚úÖ **Queue Depth Irrelevant** - No measurable difference between 32-256
4. ‚úÖ **Source IP Binding Works** - UDP sockets can bind source and still send
5. ‚úÖ **Stats are Free** - 0.12% overhead (negligible)

**Key Findings:**
- **Throughput:** 1.85M pps (batch 64) - adequate for 1:5 amplification (1.56M target)
- **Latency:** 34.6 ¬µs per 64-packet batch (< 200 ¬µs target)
- **Syscall reduction:** 32x (1,850,000 ‚Üí 57,812 syscalls/sec)
- **CPU savings:** 74% ‚Üí 2.3% CPU for egress at 1.85M pps
- **Optimal config:** Queue depth 64-128, batch size 32-64

**Gap Analysis:**
- Target for 1:10 amplification: 3.1M pps (not met - 1.85M = 59.7%)
- Target for 1:5 amplification: 1.56M pps (met with 18.6% headroom ‚úÖ)
- Loopback testing likely slower than real NICs
- For extreme 1:10 scenarios: use 2 egress workers per core or accept backpressure

**Related Designs:** D8, D5, D26

**Architectural Impact:** **HIGH** - Egress batching strategy VALIDATED ‚úÖ

**Next Steps:**
- Proceed with Phase 4 egress implementation using io_uring
- Use queue depth 64-128, batch size 32-64
- Enable stats (negligible overhead)
- Accept 1:5 as typical amplification, architect for graceful degradation

---

### 6. Netlink Socket for Interface Events (D19)

**Problem:**
Supervisor must listen for network interface state changes:
- Interface UP/DOWN (link state)
- Interface added/removed (NEWLINK/DELLINK)
- Parse Netlink messages
- Reconcile with master rule list

**Unproven Assumptions:**
- How to correctly set up Netlink socket for interface events?
- What does a UP/DOWN/NEWLINK/DELLINK message actually look like?
- How to filter for only relevant events?
- How to integrate with tokio async runtime?

**Why High Priority:**
Core resilience feature (D19). Without this, **rules don't adapt to interface changes**.

**Experiment Design:**
1. Create Netlink socket with `NETLINK_ROUTE` family
2. Subscribe to `RTMGRP_LINK` events
3. In test harness:
   - Create/delete veth pairs
   - Bring interfaces up/down with `ip link set`
4. Receive and parse Netlink messages
5. Demonstrate mapping from message ‚Üí interface name + state
6. Measure event delivery latency

**Related Designs:** D19, D21

**Architectural Impact:** Resilience, interface management

---

### 7. Priority Queuing with io_uring (D13)

**Problem:**
QoS requires priority queuing:
- Extract DSCP from IP header
- Map DSCP ‚Üí TrafficClass ‚Üí PriorityLevel
- Maintain separate egress queues per priority
- Service high-priority queue first

**Unproven Assumptions:**
- How to implement priority queuing with `io_uring`'s async model?
- Do we need separate submission queues per priority?
- How to prevent low-priority starvation while preferring high-priority?
- Performance impact of priority classification per packet?

**Why High Priority:**
QoS is explicitly designed (D13), but **implementation pattern unclear** with `io_uring`.

**Experiment Design:**
1. Simulate packet processing with two priority levels
2. Implement priority queue strategies:
   - Strict priority (always drain high queue first)
   - Weighted fair queuing (3:1 ratio)
3. Feed mixed high/low priority "packets" (just buffer IDs)
4. Submit to `io_uring` based on priority
5. Measure:
   - Latency distribution per priority class
   - Throughput balance
   - Starvation occurrence

**Related Designs:** D13, D8

**Architectural Impact:** QoS implementation, performance

---

## üü¢ Medium Priority Experiments

### 8. Consistent Hashing for Rule Distribution (D23)

**Problem:**
Rules must be assigned to cores using consistent hashing:
- Hash `(input_group, input_port)` ‚Üí core ID
- Distribution should be even
- Same rule always maps to same core (stability)

**Validation Needed:**
- Test hash function distribution (e.g., FxHash, SipHash, xxHash)
- Measure skew with realistic multicast address patterns
- Verify stability across rule adds/removes

**Experiment Design:**
1. Generate 10,000 synthetic `(group, port)` pairs
2. Test various hash functions
3. Compute distribution across 8 cores
4. Measure standard deviation (should be < 15%)

**Related Designs:** D23, D10

---

### 9. Ring Buffer Tracing Overhead (D28)

**Problem:**
Per-rule packet tracing with ring buffer:
- Pre-allocated ring buffer per core
- Conditional write on trace-enabled rules
- Must have negligible overhead when disabled

**Validation Needed:**
- Measure overhead of conditional branch (`if rule.trace_enabled`)
- Measure overhead when tracing is enabled
- Verify ring buffer wraparound behavior

**Experiment Design:**
1. Implement minimal ring buffer (1000 events)
2. Benchmark hot loop with trace point:
   - Disabled (should be free due to branch prediction)
   - Enabled (1%, 10%, 100% of packets)
3. Measure throughput degradation

**Related Designs:** D28

---

### 10. NIC Offloading Impact on AF_PACKET (D31)

**Problem:**
NIC offloads (GRO/LRO) can coalesce packets:
- `AF_PACKET` might receive artificial jumbo frames
- Can break parsing assumptions
- Must be disabled on ingress

**Validation Needed:**
- Confirm that GRO/LRO creates artificial large packets at `AF_PACKET` layer
- Test whether packets seen via `AF_PACKET` respect or bypass offloads

**Experiment Design:**
1. Create veth pair in namespace
2. Enable GRO on one end
3. Send stream of small multicast packets
4. Capture with `AF_PACKET` on GRO-enabled interface
5. Verify packet sizes seen at `AF_PACKET` layer

**Related Designs:** D31, D1

---

## Experiment Lifecycle

### When to Create

Create experiment when:
1. Technical approach is unproven
2. Performance characteristics are unknown
3. Multiple subsystems interact in non-obvious ways
4. Design decision has high risk/cost of being wrong

### Documentation

Each experiment should have:
- `experiments/poc_<feature>/README.md`
- Problem statement
- Key findings
- Architectural impact
- Run instructions
- Link to design decisions (D-numbers)

### Integration Path

After successful experiment:
1. Document findings in DEVLOG.md
2. Update ARCHITECTURE.md if design changes
3. Add to `experiments/README.md` index
4. Reference in implementation (code comments)

---

## Summary of Recommended Experiments

| Priority | Experiment | Design Decisions | Risk Area | Status |
|----------|-----------|------------------|-----------|--------|
| üî¥ Critical | Helper Socket Pattern | D6, D4, D3 | Ingress filtering | ‚úÖ VALIDATED (2025-11-07) |
| üî¥ Critical | FD Passing with Privilege Drop | D24, D1, D7 | Security architecture | ‚úÖ VALIDATED (2025-11-07) |
| üî¥ Critical | Buffer Pool Performance | D15, D16, D5 | Memory management | ‚úÖ VALIDATED (2025-11-07) |
| üü° High | Packet Parsing Performance | D3, D11, D30 | Data plane hot path | ‚è≥ Pending |
| üü° High | Batched io_uring sendto() | D8, D5, D26 | Egress performance | ‚úÖ VALIDATED (2025-11-07) |
| üü° High | Netlink Interface Events | D19, D21 | Resilience | ‚è≥ Pending |
| üü° High | Priority Queuing | D13, D8 | QoS implementation | ‚è≥ Pending |
| üü¢ Medium | Consistent Hashing | D23, D10 | Load distribution | ‚è≥ Pending |
| üü¢ Medium | Ring Buffer Tracing | D28 | Observability | ‚è≥ Pending |
| üü¢ Medium | NIC Offloading Impact | D31, D1 | Packet capture | ‚è≥ Pending |

---

**Recommendation:** ‚úÖ All critical experiments AND io_uring egress validated! Core data path fully de-risked. Ready to proceed with Phase 4 implementation or continue with remaining experiments.

**Progress:** 4/10 experiments completed (40%)
